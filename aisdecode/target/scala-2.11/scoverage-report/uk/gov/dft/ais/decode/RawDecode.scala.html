<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          uk/gov/dft/ais/decode/RawDecode.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier;'>1 <span style=''>package uk.gov.dft.ais.decode
</span>2 <span style=''>
</span>3 <span style=''>import Utils.{TimestampParse, ais_to_binary, process_checksum}
</span>4 <span style=''>import org.apache.spark.rdd.RDD
</span>5 <span style=''>import org.apache.spark.sql.{DataFrame, SparkSession}
</span>6 <span style=''>import org.apache.spark.sql.expressions.Window
</span>7 <span style=''>import org.apache.spark.sql.functions.{collect_list, concat_ws, row_number, udf, _}
</span>8 <span style=''>
</span>9 <span style=''>object RawDecode {
</span>10 <span style=''>  /**
</span>11 <span style=''>   * Decode raw AIS messages
</span>12 <span style=''>   * args - expects two arguments:
</span>13 <span style=''>   *  input file e.g. 'path/to/files/(asterix).dat'
</span>14 <span style=''>   *  output file e.g. 'path/to/output/folder/'
</span>15 <span style=''>   */
</span>16 <span style=''>  def main (args:Array[String]): Unit = {
</span>17 <span style=''>
</span>18 <span style=''>    // Start a spark context
</span>19 <span style=''>    val spark = </span><span style='background: #F0ADAD'>SparkSession
</span>20 <span style=''></span><span style='background: #F0ADAD'>      .builder()
</span>21 <span style=''></span><span style='background: #F0ADAD'>      .appName(&quot;AIS-raw-decode&quot;)
</span>22 <span style=''></span><span style='background: #F0ADAD'>      .master(&quot;yarn&quot;)
</span>23 <span style=''></span><span style='background: #F0ADAD'>      .config(&quot;spark.executor.cores&quot;, &quot;2&quot;)
</span>24 <span style=''></span><span style='background: #F0ADAD'>      .config(&quot;spark.executor.memory&quot;, &quot;1g&quot;)
</span>25 <span style=''></span><span style='background: #F0ADAD'>      .config(&quot;spark.default.parallelism&quot;, &quot;36500&quot;)
</span>26 <span style=''></span><span style='background: #F0ADAD'>      .config(&quot;spark.sql.shuffle.partitions&quot;, &quot;36500&quot;)
</span>27 <span style=''></span><span style='background: #F0ADAD'>      .getOrCreate()</span><span style=''>
</span>28 <span style=''>
</span>29 <span style=''>    // This import has to go after the spark val exists.
</span>30 <span style=''>    import spark.implicits._
</span>31 <span style=''>
</span>32 <span style=''>    // Read from the location given in args (0) as RDD
</span>33 <span style=''>    val bucket = </span><span style='background: #F0ADAD'>args(0)</span><span style=''>
</span>34 <span style=''>
</span>35 <span style=''>    val lines = </span><span style='background: #F0ADAD'>spark.sparkContext.textFile(bucket)</span><span style=''>
</span>36 <span style=''>
</span>37 <span style=''>    val all_messages = </span><span style='background: #F0ADAD'>transform(spark, lines)</span><span style=''>
</span>38 <span style=''>
</span>39 <span style=''>    // write out to a parquet
</span>40 <span style=''>    </span><span style='background: #F0ADAD'>all_messages.write.partitionBy(&quot;id&quot;).parquet(args(1))</span><span style=''>
</span>41 <span style=''>
</span>42 <span style=''>  }
</span>43 <span style=''>
</span>44 <span style=''>  def transform(spark: SparkSession, lines: RDD[String]): DataFrame = {
</span>45 <span style=''>    import spark.implicits._
</span>46 <span style=''>
</span>47 <span style=''>    // Only keep messages that pass both checksums
</span>48 <span style=''>    val passed_checksum = </span><span style='background: #AEF1AE'>lines.filter(v =&gt; process_checksum(v))</span><span style=''>
</span>49 <span style=''>
</span>50 <span style=''>    // Parse AIS strings into a structure
</span>51 <span style=''>    val ds = </span><span style='background: #AEF1AE'>passed_checksum.map(l =&gt; RawAISPacket.parseAISString(l)).toDS</span><span style=''>
</span>52 <span style=''>
</span>53 <span style=''>    // Loop through and generate a unique ID for multi part messages
</span>54 <span style=''>    // This method is tollerant of messages of any length and has been tested
</span>55 <span style=''>    // on messages between 1-3 sentences.
</span>56 <span style=''>    val window = </span><span style='background: #AEF1AE'>Window
</span>57 <span style=''></span><span style='background: #AEF1AE'>      .partitionBy($&quot;port_mmsi&quot;, $&quot;timestamp&quot;, $&quot;fragment_count&quot;)
</span>58 <span style=''></span><span style='background: #AEF1AE'>      .orderBy(&quot;fragment_n&quot;)</span><span style=''>
</span>59 <span style=''>
</span>60 <span style=''>    val multi_part_messages_with_index = </span><span style='background: #AEF1AE'>ds
</span>61 <span style=''></span><span style='background: #AEF1AE'>      .withColumn(&quot;GeneratedID&quot;, $&quot;fragment_n&quot; - row_number.over(window))</span><span style=''>
</span>62 <span style=''>
</span>63 <span style=''>    // Using the unique ID generated above, merge the messages by concatinating
</span>64 <span style=''>    // the strings
</span>65 <span style=''>    val merged_messages = </span><span style='background: #AEF1AE'>multi_part_messages_with_index
</span>66 <span style=''></span><span style='background: #AEF1AE'>      .groupBy(
</span>67 <span style=''></span><span style='background: #AEF1AE'>        &quot;packet_type&quot;,
</span>68 <span style=''></span><span style='background: #AEF1AE'>        &quot;fragment_count&quot;,
</span>69 <span style=''></span><span style='background: #AEF1AE'>        &quot;radio_channel&quot;,
</span>70 <span style=''></span><span style='background: #AEF1AE'>        &quot;padding&quot;,
</span>71 <span style=''></span><span style='background: #AEF1AE'>        &quot;s&quot;,
</span>72 <span style=''></span><span style='background: #AEF1AE'>        &quot;port&quot;,
</span>73 <span style=''></span><span style='background: #AEF1AE'>        &quot;port_mmsi&quot;,
</span>74 <span style=''></span><span style='background: #AEF1AE'>        &quot;timestamp&quot;,
</span>75 <span style=''></span><span style='background: #AEF1AE'>        &quot;GeneratedID&quot;)
</span>76 <span style=''></span><span style='background: #AEF1AE'>      .agg(concat_ws(&quot;&quot;, collect_list(&quot;data&quot;)) as &quot;data&quot;)</span><span style=''>
</span>77 <span style=''>
</span>78 <span style=''>    // Drop the Generated ID
</span>79 <span style=''>    var all_messages = </span><span style='background: #AEF1AE'>merged_messages.drop(&quot;GeneratedID&quot;)</span><span style=''>
</span>80 <span style=''>
</span>81 <span style=''>    // Convert ais message to binary string
</span>82 <span style=''>    val binary_message_udf = </span><span style='background: #AEF1AE'>udf(ais_to_binary _)</span><span style=''>
</span>83 <span style=''>    all_messages = </span><span style='background: #AEF1AE'>all_messages
</span>84 <span style=''></span><span style='background: #AEF1AE'>      .withColumn(&quot;dataBinary&quot;, binary_message_udf($&quot;data&quot;))</span><span style=''>
</span>85 <span style=''>
</span>86 <span style=''>    // Extract message type from binary string
</span>87 <span style=''>    def returnMessageType(as_binary: String): Int = {
</span>88 <span style=''>      </span><span style='background: #AEF1AE'>Integer.parseInt(as_binary.slice(0,6), 2)</span><span style=''>
</span>89 <span style=''>    }
</span>90 <span style=''>
</span>91 <span style=''>    val messageTypeUDF = </span><span style='background: #AEF1AE'>udf(returnMessageType _)</span><span style=''>
</span>92 <span style=''>    </span><span style='background: #AEF1AE'>all_messages.
</span>93 <span style=''></span><span style='background: #AEF1AE'>      withColumn(&quot;id&quot;, messageTypeUDF($&quot;dataBinary&quot;))</span><span style=''>
</span>94 <span style=''>  }
</span>95 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Code</th>
      </tr><tr>
        <td>
          27
        </td>
        <td>
          552
        </td>
        <td>
          595
          -
          895
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.SparkSession.Builder.getOrCreate
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.SparkSession.builder().appName(&quot;AIS-raw-decode&quot;).master(&quot;yarn&quot;).config(&quot;spark.executor.cores&quot;, &quot;2&quot;).config(&quot;spark.executor.memory&quot;, &quot;1g&quot;).config(&quot;spark.default.parallelism&quot;, &quot;36500&quot;).config(&quot;spark.sql.shuffle.partitions&quot;, &quot;36500&quot;).getOrCreate()
        </td>
      </tr><tr>
        <td>
          33
        </td>
        <td>
          553
        </td>
        <td>
          1056
          -
          1063
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #F0ADAD">
          args.apply(0)
        </td>
      </tr><tr>
        <td>
          35
        </td>
        <td>
          555
        </td>
        <td>
          1081
          -
          1116
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.SparkContext.textFile
        </td>
        <td style="background: #F0ADAD">
          spark.sparkContext.textFile(bucket, spark.sparkContext.textFile$default$2)
        </td>
      </tr><tr>
        <td>
          35
        </td>
        <td>
          554
        </td>
        <td>
          1100
          -
          1100
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.SparkContext.textFile$default$2
        </td>
        <td style="background: #F0ADAD">
          spark.sparkContext.textFile$default$2
        </td>
      </tr><tr>
        <td>
          37
        </td>
        <td>
          556
        </td>
        <td>
          1141
          -
          1164
        </td>
        <td>
          Apply
        </td>
        <td>
          uk.gov.dft.ais.decode.RawDecode.transform
        </td>
        <td style="background: #F0ADAD">
          RawDecode.this.transform(spark, lines)
        </td>
      </tr><tr>
        <td>
          40
        </td>
        <td>
          559
        </td>
        <td>
          1200
          -
          1253
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.DataFrameWriter.parquet
        </td>
        <td style="background: #F0ADAD">
          all_messages.write.partitionBy(&quot;id&quot;).parquet(args.apply(1))
        </td>
      </tr><tr>
        <td>
          40
        </td>
        <td>
          558
        </td>
        <td>
          1245
          -
          1252
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #F0ADAD">
          args.apply(1)
        </td>
      </tr><tr>
        <td>
          40
        </td>
        <td>
          557
        </td>
        <td>
          1231
          -
          1235
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;id&quot;
        </td>
      </tr><tr>
        <td>
          48
        </td>
        <td>
          561
        </td>
        <td>
          1439
          -
          1477
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.rdd.RDD.filter
        </td>
        <td style="background: #AEF1AE">
          lines.filter(((v: String) =&gt; Utils.process_checksum(v)))
        </td>
      </tr><tr>
        <td>
          48
        </td>
        <td>
          560
        </td>
        <td>
          1457
          -
          1476
        </td>
        <td>
          Apply
        </td>
        <td>
          uk.gov.dft.ais.decode.Utils.process_checksum
        </td>
        <td style="background: #AEF1AE">
          Utils.process_checksum(v)
        </td>
      </tr><tr>
        <td>
          51
        </td>
        <td>
          562
        </td>
        <td>
          1534
          -
          1595
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.DatasetHolder.toDS
        </td>
        <td style="background: #AEF1AE">
          spark.implicits.rddToDatasetHolder[uk.gov.dft.ais.decode.RawAISPacket](passed_checksum.map[uk.gov.dft.ais.decode.RawAISPacket](((l: String) =&gt; RawAISPacket.parseAISString(l)))((ClassTag.apply[uk.gov.dft.ais.decode.RawAISPacket](classOf[uk.gov.dft.ais.decode.RawAISPacket]): scala.reflect.ClassTag[uk.gov.dft.ais.decode.RawAISPacket])))(spark.implicits.newProductEncoder[uk.gov.dft.ais.decode.RawAISPacket](({
  val $u: reflect.runtime.universe.type = scala.reflect.runtime.`package`.universe;
  val $m: $u.Mirror = scala.reflect.runtime.`package`.universe.runtimeMirror(this.getClass().getClassLoader());
  $u.TypeTag.apply[uk.gov.dft.ais.decode.RawAISPacket]($m, {
    final class $typecreator5 extends TypeCreator {
      def &lt;init&gt;(): $typecreator5 = {
        $typecreator5.super.&lt;init&gt;();
        ()
      };
      def apply[U &lt;: scala.reflect.api.Universe with Singleton]($m$untyped: scala.reflect.api.Mirror[U]): U#Type = {
        val $u: U = $m$untyped.universe;
        val $m: $u.Mirror = $m$untyped.asInstanceOf[$u.Mirror];
        $m.staticClass(&quot;uk.gov.dft.ais.decode.RawAISPacket&quot;).asType.toTypeConstructor
      }
    };
    new $typecreator5()
  })
}: reflect.runtime.universe.TypeTag[uk.gov.dft.ais.decode.RawAISPacket]))).toDS()
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          563
        </td>
        <td>
          1803
          -
          1904
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.expressions.WindowSpec.orderBy
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.expressions.Window.partitionBy(spark.implicits.StringToColumn(scala.StringContext.apply(&quot;port_mmsi&quot;)).$(), spark.implicits.StringToColumn(scala.StringContext.apply(&quot;timestamp&quot;)).$(), spark.implicits.StringToColumn(scala.StringContext.apply(&quot;fragment_count&quot;)).$()).orderBy(&quot;fragment_n&quot;)
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          568
        </td>
        <td>
          1947
          -
          2023
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.Dataset.withColumn
        </td>
        <td style="background: #AEF1AE">
          ds.withColumn(&quot;GeneratedID&quot;, spark.implicits.StringToColumn(scala.StringContext.apply(&quot;fragment_n&quot;)).$().-(org.apache.spark.sql.functions.row_number().over(window)))
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          567
        </td>
        <td>
          1983
          -
          2022
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.Column.-
        </td>
        <td style="background: #AEF1AE">
          spark.implicits.StringToColumn(scala.StringContext.apply(&quot;fragment_n&quot;)).$().-(org.apache.spark.sql.functions.row_number().over(window))
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          564
        </td>
        <td>
          1968
          -
          1981
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;GeneratedID&quot;
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          566
        </td>
        <td>
          1999
          -
          2022
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.Column.over
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.row_number().over(window)
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          565
        </td>
        <td>
          1983
          -
          1996
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.apply
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;fragment_n&quot;)
        </td>
      </tr><tr>
        <td>
          67
        </td>
        <td>
          569
        </td>
        <td>
          2205
          -
          2218
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;packet_type&quot;
        </td>
      </tr><tr>
        <td>
          68
        </td>
        <td>
          570
        </td>
        <td>
          2228
          -
          2244
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;fragment_count&quot;
        </td>
      </tr><tr>
        <td>
          69
        </td>
        <td>
          571
        </td>
        <td>
          2254
          -
          2269
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;radio_channel&quot;
        </td>
      </tr><tr>
        <td>
          70
        </td>
        <td>
          572
        </td>
        <td>
          2279
          -
          2288
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;padding&quot;
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          573
        </td>
        <td>
          2298
          -
          2301
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;s&quot;
        </td>
      </tr><tr>
        <td>
          72
        </td>
        <td>
          574
        </td>
        <td>
          2311
          -
          2317
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;port&quot;
        </td>
      </tr><tr>
        <td>
          73
        </td>
        <td>
          575
        </td>
        <td>
          2327
          -
          2338
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;port_mmsi&quot;
        </td>
      </tr><tr>
        <td>
          74
        </td>
        <td>
          576
        </td>
        <td>
          2348
          -
          2359
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;timestamp&quot;
        </td>
      </tr><tr>
        <td>
          75
        </td>
        <td>
          577
        </td>
        <td>
          2369
          -
          2382
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;GeneratedID&quot;
        </td>
      </tr><tr>
        <td>
          76
        </td>
        <td>
          579
        </td>
        <td>
          2150
          -
          2441
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.RelationalGroupedDataset.agg
        </td>
        <td style="background: #AEF1AE">
          multi_part_messages_with_index.groupBy(&quot;packet_type&quot;, &quot;fragment_count&quot;, &quot;radio_channel&quot;, &quot;padding&quot;, &quot;s&quot;, &quot;port&quot;, &quot;port_mmsi&quot;, &quot;timestamp&quot;, &quot;GeneratedID&quot;).agg(org.apache.spark.sql.functions.concat_ws(&quot;&quot;, org.apache.spark.sql.functions.collect_list(&quot;data&quot;)).as(&quot;data&quot;))
        </td>
      </tr><tr>
        <td>
          76
        </td>
        <td>
          578
        </td>
        <td>
          2395
          -
          2440
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.Column.as
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.concat_ws(&quot;&quot;, org.apache.spark.sql.functions.collect_list(&quot;data&quot;)).as(&quot;data&quot;)
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          580
        </td>
        <td>
          2495
          -
          2530
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.Dataset.drop
        </td>
        <td style="background: #AEF1AE">
          merged_messages.drop(&quot;GeneratedID&quot;)
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          582
        </td>
        <td>
          2605
          -
          2625
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          org.apache.spark.sql.functions.udf
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.udf[String, String]({
  ((encodedMessage: String) =&gt; Utils.ais_to_binary(encodedMessage))
})(({
  val $u: reflect.runtime.universe.type = scala.reflect.runtime.`package`.universe;
  val $m: $u.Mirror = scala.reflect.runtime.`package`.universe.runtimeMirror(this.getClass().getClassLoader());
  $u.TypeTag.apply[String]($m, {
    final class $typecreator9 extends TypeCreator {
      def &lt;init&gt;(): $typecreator9 = {
        $typecreator9.super.&lt;init&gt;();
        ()
      };
      def apply[U &lt;: scala.reflect.api.Universe with Singleton]($m$untyped: scala.reflect.api.Mirror[U]): U#Type = {
        val $u: U = $m$untyped.universe;
        val $m: $u.Mirror = $m$untyped.asInstanceOf[$u.Mirror];
        $u.internal.reificationSupport.TypeRef($u.internal.reificationSupport.SingleType($u.internal.reificationSupport.ThisType($m.staticPackage(&quot;scala&quot;).asModule.moduleClass), $m.staticModule(&quot;scala.Predef&quot;)), $u.internal.reificationSupport.selectType($m.staticModule(&quot;scala.Predef&quot;).asModule.moduleClass, &quot;String&quot;), immutable.this.Nil)
      }
    };
    new $typecreator9()
  })
}: reflect.runtime.universe.TypeTag[String]), ({
  val $u: reflect.runtime.universe.type = scala.reflect.runtime.`package`.universe;
  val $m: $u.Mirror = scala.reflect.runtime.`package`.universe.runtimeMirror(this.getClass().getClassLoader());
  $u.TypeTag.apply[String]($m, {
    final class $typecreator10 extends TypeCreator {
      def &lt;init&gt;(): $typecreator10 = {
        $typecreator10.super.&lt;init&gt;();
        ()
      };
      def apply[U &lt;: scala.reflect.api.Universe with Singleton]($m$untyped: scala.reflect.api.Mirror[U]): U#Type = {
        val $u: U = $m$untyped.universe;
        val $m: $u.Mirror = $m$untyped.asInstanceOf[$u.Mirror];
        $u.internal.reificationSupport.TypeRef($u.internal.reificationSupport.SingleType($u.internal.reificationSupport.ThisType($m.staticPackage(&quot;scala&quot;).asModule.moduleClass), $m.staticModule(&quot;scala.Predef&quot;)), $u.internal.reificationSupport.selectType($m.staticModule(&quot;scala.Predef&quot;).asModule.moduleClass, &quot;String&quot;), immutable.this.Nil)
      }
    };
    new $typecreator10()
  })
}: reflect.runtime.universe.TypeTag[String]))
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          581
        </td>
        <td>
          2609
          -
          2622
        </td>
        <td>
          Apply
        </td>
        <td>
          uk.gov.dft.ais.decode.Utils.ais_to_binary
        </td>
        <td style="background: #AEF1AE">
          Utils.ais_to_binary(encodedMessage)
        </td>
      </tr><tr>
        <td>
          84
        </td>
        <td>
          585
        </td>
        <td>
          2690
          -
          2717
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.expressions.UserDefinedFunction.apply
        </td>
        <td style="background: #AEF1AE">
          binary_message_udf.apply(spark.implicits.StringToColumn(scala.StringContext.apply(&quot;data&quot;)).$())
        </td>
      </tr><tr>
        <td>
          84
        </td>
        <td>
          584
        </td>
        <td>
          2709
          -
          2716
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.SQLImplicits.StringToColumn.$
        </td>
        <td style="background: #AEF1AE">
          spark.implicits.StringToColumn(scala.StringContext.apply(&quot;data&quot;)).$()
        </td>
      </tr><tr>
        <td>
          84
        </td>
        <td>
          586
        </td>
        <td>
          2645
          -
          2718
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.Dataset.withColumn
        </td>
        <td style="background: #AEF1AE">
          all_messages.withColumn(&quot;dataBinary&quot;, binary_message_udf.apply(spark.implicits.StringToColumn(scala.StringContext.apply(&quot;data&quot;)).$()))
        </td>
      </tr><tr>
        <td>
          84
        </td>
        <td>
          583
        </td>
        <td>
          2676
          -
          2688
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;dataBinary&quot;
        </td>
      </tr><tr>
        <td>
          88
        </td>
        <td>
          588
        </td>
        <td>
          2866
          -
          2867
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          88
        </td>
        <td>
          587
        </td>
        <td>
          2844
          -
          2864
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.immutable.StringOps.slice
        </td>
        <td style="background: #AEF1AE">
          scala.this.Predef.augmentString(as_binary).slice(0, 6)
        </td>
      </tr><tr>
        <td>
          88
        </td>
        <td>
          589
        </td>
        <td>
          2827
          -
          2868
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Integer.parseInt
        </td>
        <td style="background: #AEF1AE">
          java.this.lang.Integer.parseInt(scala.this.Predef.augmentString(as_binary).slice(0, 6), 2)
        </td>
      </tr><tr>
        <td>
          91
        </td>
        <td>
          591
        </td>
        <td>
          2901
          -
          2925
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          org.apache.spark.sql.functions.udf
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.udf[Int, String]({
  ((as_binary: String) =&gt; returnMessageType(as_binary))
})((scala.reflect.runtime.`package`.universe.TypeTag.Int: reflect.runtime.universe.TypeTag[Int]), ({
  val $u: reflect.runtime.universe.type = scala.reflect.runtime.`package`.universe;
  val $m: $u.Mirror = scala.reflect.runtime.`package`.universe.runtimeMirror(this.getClass().getClassLoader());
  $u.TypeTag.apply[String]($m, {
    final class $typecreator11 extends TypeCreator {
      def &lt;init&gt;(): $typecreator11 = {
        $typecreator11.super.&lt;init&gt;();
        ()
      };
      def apply[U &lt;: scala.reflect.api.Universe with Singleton]($m$untyped: scala.reflect.api.Mirror[U]): U#Type = {
        val $u: U = $m$untyped.universe;
        val $m: $u.Mirror = $m$untyped.asInstanceOf[$u.Mirror];
        $u.internal.reificationSupport.TypeRef($u.internal.reificationSupport.SingleType($u.internal.reificationSupport.ThisType($m.staticPackage(&quot;scala&quot;).asModule.moduleClass), $m.staticModule(&quot;scala.Predef&quot;)), $u.internal.reificationSupport.selectType($m.staticModule(&quot;scala.Predef&quot;).asModule.moduleClass, &quot;String&quot;), immutable.this.Nil)
      }
    };
    new $typecreator11()
  })
}: reflect.runtime.universe.TypeTag[String]))
        </td>
      </tr><tr>
        <td>
          91
        </td>
        <td>
          590
        </td>
        <td>
          2905
          -
          2922
        </td>
        <td>
          Apply
        </td>
        <td>
          uk.gov.dft.ais.decode.RawDecode.returnMessageType
        </td>
        <td style="background: #AEF1AE">
          returnMessageType(as_binary)
        </td>
      </tr><tr>
        <td>
          93
        </td>
        <td>
          594
        </td>
        <td>
          2967
          -
          2996
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.expressions.UserDefinedFunction.apply
        </td>
        <td style="background: #AEF1AE">
          messageTypeUDF.apply(spark.implicits.StringToColumn(scala.StringContext.apply(&quot;dataBinary&quot;)).$())
        </td>
      </tr><tr>
        <td>
          93
        </td>
        <td>
          593
        </td>
        <td>
          2982
          -
          2995
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.SQLImplicits.StringToColumn.$
        </td>
        <td style="background: #AEF1AE">
          spark.implicits.StringToColumn(scala.StringContext.apply(&quot;dataBinary&quot;)).$()
        </td>
      </tr><tr>
        <td>
          93
        </td>
        <td>
          595
        </td>
        <td>
          2930
          -
          2997
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.Dataset.withColumn
        </td>
        <td style="background: #AEF1AE">
          all_messages.withColumn(&quot;id&quot;, messageTypeUDF.apply(spark.implicits.StringToColumn(scala.StringContext.apply(&quot;dataBinary&quot;)).$()))
        </td>
      </tr><tr>
        <td>
          93
        </td>
        <td>
          592
        </td>
        <td>
          2961
          -
          2965
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;id&quot;
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>